1. Voraussetzungen
Bevor du Apache Spark installierst, stelle sicher, dass folgende Komponenten auf deinem System installiert sind:

Java Development Kit (JDK): Spark benötigt Java. Installiere OpenJDK:
sudo apt update
sudo apt install openjdk-11-jdk -y
Überprüfe die Installation:

java -version
Python (optional): Wenn du Spark mit Python verwenden möchtest:
sudo apt install python3 python3-pip -y
2. Apache Spark herunterladen
Besuche die offizielle Apache Spark-Download-Seite und kopiere den Link zur gewünschten Version.
Lade Spark herunter:
wget https://downloads.apache.org/spark/spark-3.4.2/spark-3.4.2-bin-hadoop3.tgz
Entpacke die Datei:
tar -xvzf spark-3.4.2-bin-hadoop3.tgz
Verschiebe den entpackten Ordner an einen geeigneten Ort, z. B. /opt:
sudo mv spark-3.4.2-bin-hadoop3 /opt/spark
3. Umgebungsvariablen einrichten
Öffne die Datei .bashrc oder .zshrc (je nachdem, welche Shell du nutzt):
nano ~/.bashrc
Füge folgende Zeilen am Ende der Datei hinzu:
export SPARK_HOME=/opt/spark
export PATH=$SPARK_HOME/bin:$PATH
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
Lade die Änderungen neu:
source ~/.bashrc
4. Spark testen
Überprüfe, ob Spark funktioniert:
spark-shell
Dies startet die Spark-Shell (Scala). Wenn du das Spark-Banner siehst, ist die Installation erfolgreich.
Python-Shell (PySpark): Wenn du mit Python arbeiten möchtest, starte PySpark:
pyspark
5. Spark als Cluster einrichten (optional)
Falls du Spark in einem Cluster-Modus verwenden möchtest:

Konfiguriere Spark als Standalone-Cluster:
Öffne die Datei spark-env.sh:
nano /opt/spark/conf/spark-env.sh
Füge folgende Zeilen hinzu:
SPARK_MASTER_HOST='127.0.0.1'
Starten von Spark Master und Worker:
Master starten:
$SPARK_HOME/sbin/start-master.sh
Worker starten:
$SPARK_HOME/sbin/start-worker.sh spark://127.0.0.1:7077
Web-UI: Öffne die Spark-Weboberfläche unter:
http://<deine-ip>:8080
